{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Exercise 1: function to match a regex against a string\n",
    "# returns: true if the whole string matches the pattern, false o/w\n",
    "def regexMatch(pattern, string):\n",
    "   matches = re.match(pattern, string, re.UNICODE)\n",
    "   return True if matches else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa:  True\n",
      "Aaa:  True\n",
      "aAa:  True\n",
      "123:  False\n",
      "a12:  False\n",
      "12a:  False\n",
      "Aa!:  False\n",
      "   :  False\n",
      "a a:  False\n"
     ]
    }
   ],
   "source": [
    "# 1a: Regex for all alphabetic strings\n",
    "pattern = \"^[a-zA-Z]+$\"\n",
    "\n",
    "print(\"aaa: \", regexMatch(pattern, \"aaa\"))\n",
    "print(\"Aaa: \", regexMatch(pattern, \"Aaa\"))\n",
    "print(\"aAa: \", regexMatch(pattern, \"aAa\"))\n",
    "print(\"123: \", regexMatch(pattern, \"123\"))\n",
    "print(\"a12: \", regexMatch(pattern, \"a12\"))\n",
    "print(\"12a: \", regexMatch(pattern, \"12a\"))\n",
    "print(\"Aa!: \", regexMatch(pattern, \"Aa!\"))\n",
    "print(\"   : \", regexMatch(pattern, \"   \"))\n",
    "print(\"a a: \", regexMatch(pattern, \"a a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:     True\n",
      "abcs:  True\n",
      "aAas:  False\n",
      "aAAa:  False\n",
      "a12s:  False\n",
      "12as:  False\n",
      "Aa!s:  False\n",
      "s   :  False\n",
      "   s:  False\n"
     ]
    }
   ],
   "source": [
    "# 1b: Regex for all lowercase strings ending in s\n",
    "pattern = \"^[a-z]*s$\"\n",
    "\n",
    "print(\"s:    \", regexMatch(pattern, \"s\"))\n",
    "print(\"abcs: \", regexMatch(pattern, \"abcs\"))\n",
    "print(\"aAas: \", regexMatch(pattern, \"aAas\"))\n",
    "print(\"aAAa: \", regexMatch(pattern, \"aAAa\"))\n",
    "print(\"a12s: \", regexMatch(pattern, \"a12s\"))\n",
    "print(\"12as: \", regexMatch(pattern, \"12as\"))\n",
    "print(\"Aa!s: \", regexMatch(pattern, \"Aa!s\"))\n",
    "print(\"s   : \", regexMatch(pattern, \"s   \"))\n",
    "print(\"   s: \", regexMatch(pattern, \"   s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c.\n",
    "\n",
    "NOTE: this is not a regular language, and thus this effect cannot be achieved by a regular expression on its own\n",
    "We need a regex, for its back-referencing features. \n",
    "The question, as it was clarified in the lecture, asks for the pattern to be case-insensitive for the first letter\n",
    "only. This cannot be achieved in python, short of listing all possible first letters - python allows for the whole\n",
    "word to be case sensitive or case insensitive, but not parts of it. \n",
    "Thus the presented pattern will match case-insensitively: \"mirror mirror\", \"Mirror mirror\", and \"miRRor mirror\" will all match. \n",
    "\n",
    "To get case insensitive first letter only, use the following pattern in Perl: \n",
    "\n",
    "`\"(/([A-Za-z])([A-Za-z]*) (?i)\\1(?-i)\\2/)\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa aa:         True\n",
      "Aaa aaa:       True\n",
      "duck ducks:    False\n",
      "11 11:         False\n",
      "11 aa aa:      True\n",
      "the big bang:  False\n",
      "a aa aa a:     True\n",
      "duckduck:      False\n",
      "4G 4G:         False\n"
     ]
    }
   ],
   "source": [
    "# 1c: Regex for all strings with two consecutive words being the same (case insensitive on the first letter)\n",
    "\n",
    "pattern = r\"(.*\\W?)(?i)([A-Za-z]+) \\2([^a-zA-Z]|$)\"\n",
    "\n",
    "print(\"aa aa:        \", regexMatch(pattern, \"aa aa\"))\n",
    "print(\"Aaa aaa:      \", regexMatch(pattern, \"Aaa aaa\"))\n",
    "print(\"duck ducks:   \", regexMatch(pattern, \"duck ducks\"))\n",
    "print(\"11 11:        \", regexMatch(pattern, \"11 11\"))\n",
    "print(\"11 aa aa:     \", regexMatch(pattern, \"11 aa aa\"))\n",
    "print(\"the big bang: \", regexMatch(pattern, \"the big bang\"))\n",
    "print(\"a aa aa a:    \", regexMatch(pattern, \"a aa aa a\"))\n",
    "print(\"duckduck:     \", regexMatch(pattern, \"duckduck\"))\n",
    "print(\"4G 4G:        \", regexMatch(pattern, \"4G 4G\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 aaa:       True\n",
      "11 345 aa:   True\n",
      "11 345! aa:  True\n",
      " 1 aa:       False\n",
      "123:         False\n",
      "3.14 aa:     False\n",
      "4G abc:      False\n",
      "42 4G:       False\n",
      "4G:          False\n"
     ]
    }
   ],
   "source": [
    "# 1d: Regex for all strings that start at a new line with an integer and ends at the end of a line with a word\n",
    "\n",
    "# here we take \"line\" to be synonymous with \"string\" - i.e. not looking for new line characters\n",
    "\n",
    "pattern = \"^[0-9]+\\s(.*\\s)?[a-zA-Z]+$\"\n",
    "\n",
    "print(\"1 aaa:      \", regexMatch(pattern, \"1 aaa\"))\n",
    "print(\"11 345 aa:  \", regexMatch(pattern, \"11 345 aa\"))\n",
    "print(\"11 345! aa: \", regexMatch(pattern, \"11 345 aa\"))\n",
    "print(\" 1 aa:      \", regexMatch(pattern, \" 1 aa\"))\n",
    "print(\"123:        \", regexMatch(pattern, \"123\"))\n",
    "print(\"3.14 aa:    \", regexMatch(pattern, \"3.14 aa\"))\n",
    "print(\"4G abc:     \", regexMatch(pattern, \"4G abc\"))\n",
    "print(\"42 4G:      \", regexMatch(pattern, \"42 4G\"))\n",
    "print(\"4G:         \", regexMatch(pattern, \"4G\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaa sbb like duck:  True\n",
      "likes duck:          True\n",
      "ducklike:            True\n",
      "23323duck243like:    True\n",
      "    duck:            False\n",
      "like:                False\n",
      "aasd asd 2342 @#$ :  False\n"
     ]
    }
   ],
   "source": [
    "# 1e: Regex for all strings that contain the words like and duck\n",
    "# here we also allow for 'like' and 'duck' to be substrings - e.g. 'likes duck' will match\n",
    "# if that is not intended, the pattern should be: \"((.*\\W)?like\\W(.*\\W)?duck\\W.*)|((.*\\W)?duck\\W(.*\\W)?like\\W.*)\"\n",
    "\n",
    "pattern = \"(.*like.*duck.*)|(.*duck.*like.*)\"\n",
    "\n",
    "print(\"aaaa sbb like duck: \", regexMatch(pattern, \"aaaa sbb like duck\"))\n",
    "print(\"likes duck:         \", regexMatch(pattern, \"likes duck\"))\n",
    "print(\"ducklike:           \", regexMatch(pattern, \"ducklike\"))\n",
    "print(\"23323duck243like:   \", regexMatch(pattern, \"23323duck243like\"))\n",
    "print(\"    duck:           \", regexMatch(pattern, \"    duck\"))\n",
    "print(\"like:               \", regexMatch(pattern, \"like\"))\n",
    "print(\"aasd asd 2342 @#$ : \", regexMatch(pattern, \"aasd asd 2342 @#$ \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# 1f: Pattern that matches the first word of all sentences in the complete works of Shakespeare\n",
    "\n",
    "# Here we take a sentence to begin with a capital letter, and end with one of: '.' '?' !' '...' ':' ';'\n",
    "# which is then followed by a white space. This WILL capture character names in the plays.\n",
    "# We are not considering ',' as an end of sentence character, because that would capture almost all words \n",
    "# at the beginning of a line.\n",
    "pattern = \"(?<=[0-9.?!:;]\\s)\\s*[A-Z][a-z]*\"\n",
    "\n",
    "# open the Shakespeare corpus\n",
    "shakes = open('shakes.txt', 'r')\n",
    "shakes_text = shakes.read()\n",
    "shakes.close()\n",
    "# open a file for writing. If the file exists, its contents will be erased\n",
    "file = open('shakespeare_first_words.txt', 'w')\n",
    "\n",
    "p = re.compile(pattern)\n",
    "results = p.findall(shakes_text)\n",
    "# We skip the first 46 sentences, because they are not the works of Shakespeare; it's the copyright notice and\n",
    "# the preamble, which are written in an inconsistent style (all caps, decorative punctuation, etc.)\n",
    "for i in range(46, len(results)):\n",
    "    file.write(results[i].lstrip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe(iii):  True\n",
      "(CH3)2CH:  True\n",
      "[Fe(Rtpen)(η2-OO)]+:  True\n",
      "[FeIIIOOH]2+:  True\n",
      "123:  False\n",
      "cat:  False\n"
     ]
    }
   ],
   "source": [
    "# First we design a regex to capture chemical formulae. \n",
    "# The strings will contain upper and lowe case letters, and symbols (, ), [, ], +, -, and Greek letters.i\n",
    "# To avoid capturing normal words, we require at least one non-letter character\n",
    "\n",
    "pattern = \"[a-zA-Z\\(\\)+\\[\\]-]+[0-9\\(\\)+\\[\\]-][a-zA-Z0-9\\(\\)+\\[\\]-]*\"\n",
    "\n",
    "# Here are some tests:\n",
    "tests = [\"Fe(iii)\", \"(CH3)2CH\", \"[Fe(Rtpen)(η2-OO)]+\", \"[FeIIIOOH]2+\", \"123\", \"cat\"]\n",
    "for i in range(len(tests)):\n",
    "    print(tests[i] + \": \", regexMatch(pattern, tests[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file b103844n_mode2.Andrew.xml:\t 176\n",
      "file b105514n_mode2.Andrew.xml:\t 91\n",
      "file b107078a_mode2.Denis.xml:\t 150\n",
      "file b108236c_mode2.Loris.xml:\t 194\n",
      "file b109309f_mode2.Nick.xml:\t 112\n",
      "file b110865b_mode2.Nick.xml:\t 169\n",
      "file b209460f_mode2.Donna.xml:\t 50\n",
      "file b210911e_mode2.Denis.xml:\t 44\n",
      "file b211806h_mode2.Nick.xml:\t 29\n",
      "file b301675g_mode2.Loris.xml:\t 21\n",
      "file b302761a_mode2.Denis.xml:\t 61\n",
      "file b303117a_mode2.Nick.xml:\t 105\n",
      "file b303244b_mode2.Andrew.xml:\t 56\n",
      "file b303587p_mode2.Filip.xml:\t 37\n",
      "file b303919f_mode2.Filip.xml:\t 34\n",
      "file b304116f_mode2.Filip.xml:\t 90\n",
      "file b304938h_mode2.Donna.xml:\t 86\n",
      "file b304943d_mode2.Loris.xml:\t 121\n",
      "file b304951e_mode2.Nick.xml:\t 105\n",
      "file b305156k_mode2.Denis.xml:\t 125\n",
      "file b305411j_mode2.Loris.xml:\t 30\n",
      "file b305670h_mode2.Paola.xml:\t 12\n",
      "file b305738k_mode2.Loris.xml:\t 66\n",
      "file b306564b_mode2.Filip.xml:\t 79\n",
      "file b306684c_mode2.Filip.xml:\t 24\n",
      "file b306702e_mode2.Petr.xml:\t 84\n",
      "file b306787b_mode2.Nick.xml:\t 49\n",
      "file b307591e_mode2.Loris.xml:\t 101\n",
      "file b308032n_mode2.Donna.xml:\t 140\n",
      "file b308195h_mode2.Filip.xml:\t 22\n",
      "file b308201f_mode2.Nick.xml:\t 136\n",
      "file b308202d_mode2.Paola.xml:\t 134\n",
      "file b308699b_mode2.Andrew.xml:\t 75\n",
      "file b309070a_mode2.Andrew.xml:\t 68\n",
      "file b309235f_mode2.Andrew.xml:\t 194\n",
      "file b309237b_mode2.Loris.xml:\t 75\n",
      "file b309357n_mode2.Denis.xml:\t 77\n",
      "file b309893a_mode2.Denis.xml:\t 80\n",
      "file b309964d_mode2.Paola.xml:\t 48\n",
      "file b310125h_mode2.Filip.xml:\t 78\n",
      "file b310238f_mode2.Denis.xml:\t 107\n",
      "file b310282c_mode2.Petr.xml:\t 32\n",
      "file b310495h_mode2.Nick.xml:\t 70\n",
      "file b310655a_mode2.Filip.xml:\t 42\n",
      "file b310723j_mode2.Denis.xml:\t 38\n",
      "file b310767a_mode2.Filip.xml:\t 145\n",
      "file b310806f_mode2.Loris.xml:\t 49\n",
      "file b310825b_mode2.Donna.xml:\t 75\n",
      "file b310850c_mode2.Filip.xml:\t 173\n",
      "file b310851a_mode2.Kamran.xml:\t 29\n",
      "file b310865a_mode2.Loris.xml:\t 55\n",
      "file b310932a_mode2.Denis.xml:\t 37\n",
      "file b310952f_mode2.Nick.xml:\t 47\n",
      "file b311197k_mode2.Andrew.xml:\t 25\n",
      "file b311254c_mode2.Andrew.xml:\t 36\n",
      "file b311304c_mode2.Denis.xml:\t 82\n",
      "file b311571b_mode2.Loris.xml:\t 34\n",
      "file b311582h_mode2.Petr.xml:\t 217\n",
      "file b311589e_mode2.Filip.xml:\t 21\n",
      "file b311656e_mode2.Andrew.xml:\t 189\n",
      "file b311934c_mode2.Denis.xml:\t 29\n",
      "file b312040f_mode2.Petr.xml:\t 54\n",
      "file b312115a_mode2.Kamran.xml:\t 88\n",
      "file b312175e_mode2.Donna.xml:\t 39\n",
      "file b312189e_mode2.Paola.xml:\t 87\n",
      "file b312196h_mode2.Loris.xml:\t 52\n",
      "file b312242e_mode2.Donna.xml:\t 145\n",
      "file b312252b_mode2.Loris.xml:\t 58\n",
      "file b312329d_mode2.Loris.xml:\t 46\n",
      "file b312562a_mode2.Donna.xml:\t 95\n",
      "file b312620j_mode2.Filip.xml:\t 70\n",
      "file b312684f_mode2.Donna.xml:\t 48\n",
      "file b312724a_mode2.Andrew.xml:\t 52\n",
      "file b312769a_mode2.Loris.xml:\t 26\n",
      "file b312859h_mode2.Donna.xml:\t 56\n",
      "file b312984e_mode2.Petr.xml:\t 30\n",
      "file b313094k_mode2.Nick.xml:\t 93\n",
      "file b313110f_mode2.Paola.xml:\t 189\n",
      "file b313134c_mode2.Petr.xml:\t 132\n",
      "file b313220j_mode2.Filip.xml:\t 160\n",
      "file b313251j_mode2.Denis.xml:\t 120\n",
      "file b313261g_mode2.Paola.xml:\t 63\n",
      "file b313306k_mode2.Kamran.xml:\t 26\n",
      "file b313313n_mode2.Petr.xml:\t 67\n",
      "file b313354k_mode2.Paola.xml:\t 42\n",
      "file b313362a_mode2.Paola.xml:\t 35\n",
      "file b313399k_mode2.Paola.xml:\t 49\n",
      "file b313474a_mode2.Donna.xml:\t 79\n",
      "file b313684a_mode2.Filip.xml:\t 71\n",
      "file b313691d_mode2.Petr.xml:\t 96\n",
      "file b313717a_mode2.Kamran.xml:\t 88\n",
      "file b313781c_mode2.Paola.xml:\t 72\n",
      "file b313883f_mode2.Filip.xml:\t 152\n",
      "file b313922k_mode2.Andrew.xml:\t 81\n",
      "file b313987e_mode2.Donna.xml:\t 64\n",
      "file b314070a_mode2.Kamran.xml:\t 94\n",
      "file b314077f_mode2.Nick.xml:\t 86\n",
      "file b314140c_mode2.Nick.xml:\t 31\n",
      "file b314172a_mode2.Loris.xml:\t 120\n",
      "file b314189f_mode2.Kamran.xml:\t 100\n",
      "file b314263a_mode2.Petr.xml:\t 29\n",
      "file b314320a_mode2.Loris.xml:\t 81\n",
      "file b314333n_mode2.Loris.xml:\t 66\n",
      "file b314477a_mode2.Loris.xml:\t 47\n",
      "file b314521m_mode2.Kamran.xml:\t 56\n",
      "file b314609j_mode2.Paola.xml:\t 80\n",
      "file b314644h_mode2.Denis.xml:\t 119\n",
      "file b314686c_mode2.Nick.xml:\t 101\n",
      "file b314692h_mode2.Nick.xml:\t 74\n",
      "file b314764a_mode2.Nick.xml:\t 42\n",
      "file b314882c_mode2.Nick.xml:\t 51\n",
      "file b315019d_mode2.Filip.xml:\t 91\n",
      "file b315034h_mode2.Nick.xml:\t 52\n",
      "file b315038k_mode2.Nick.xml:\t 19\n",
      "file b315069k_mode2.Paola.xml:\t 130\n",
      "file b315071b_mode2.Donna.xml:\t 118\n",
      "file b315089e_mode2.Denis.xml:\t 82\n",
      "file b315252a_mode2.Paola.xml:\t 125\n",
      "file b315394k_mode2.Andrew.xml:\t 60\n",
      "file b315520j_mode2.Donna.xml:\t 198\n",
      "file b315626p_mode2.Nick.xml:\t 135\n",
      "file b315695h_mode2.Denis.xml:\t 30\n",
      "file b315777f_mode2.Loris.xml:\t 65\n",
      "file b315893d_mode2.Petr.xml:\t 110\n",
      "file b316053j_mode2.Nick.xml:\t 74\n",
      "file b316144g_mode2.Loris.xml:\t 88\n",
      "file b316166h_mode2.Andrew.xml:\t 57\n",
      "file b316185d_mode2.Loris.xml:\t 137\n",
      "file b316211g_mode2.Nick.xml:\t 86\n",
      "file b316260e_mode2.Kamran.xml:\t 90\n",
      "file b316385g_mode2.Denis.xml:\t 33\n",
      "file b316402k_mode2.Denis.xml:\t 85\n",
      "file b316514k_mode2.Donna.xml:\t 63\n",
      "file b316620a_mode2.Filip.xml:\t 38\n",
      "file b316895f_mode2.Kamran.xml:\t 105\n",
      "file b316910c_mode2.Donna.xml:\t 74\n",
      "file b400183d_mode2.Paola.xml:\t 135\n",
      "file b400243a_mode2.Loris.xml:\t 87\n",
      "file b400402g_mode2.Paola.xml:\t 137\n",
      "file b400471j_mode2.Filip.xml:\t 65\n",
      "file b401234h_mode2.Paola.xml:\t 91\n",
      "file b401299b_mode2.Petr.xml:\t 64\n",
      "file b402163k_mode2.Loris.xml:\t 59\n",
      "file b402202e_mode2.Loris.xml:\t 69\n",
      "file b402410a_mode2.Filip.xml:\t 116\n",
      "file b402496f_mode2.Filip.xml:\t 102\n",
      "file b402614d_mode2.Paola.xml:\t 41\n",
      "file b402623c_mode2.Petr.xml:\t 122\n",
      "file b402989p_mode2.Filip.xml:\t 66\n",
      "file b403032j_mode2.Andrew.xml:\t 40\n",
      "file b403071k_mode2.Filip.xml:\t 161\n",
      "file b403094j_mode2.Filip.xml:\t 52\n",
      "file b403127j_mode2.Denis.xml:\t 116\n",
      "file b403191c_mode2.Donna.xml:\t 119\n",
      "file b403450c_mode2.Filip.xml:\t 63\n",
      "file b403534h_mode2.Petr.xml:\t 68\n",
      "file b403898c_mode2.Kamran.xml:\t 54\n",
      "file b403970j_mode2.Filip.xml:\t 271\n",
      "file b404078n_mode2.Kamran.xml:\t 50\n",
      "file b404146a_mode2.Donna.xml:\t 32\n",
      "file b404176c_mode2.Nick.xml:\t 162\n",
      "file b404604h_mode2.Nick.xml:\t 152\n",
      "file b404632c_mode2.Paola.xml:\t 89\n",
      "file b404633a_mode2.Donna.xml:\t 72\n",
      "file b404880f_mode2.Petr.xml:\t 46\n",
      "file b404889j_mode2.Nick.xml:\t 205\n",
      "file b404933k_mode2.Loris.xml:\t 164\n",
      "file b405013d_mode2.Kamran.xml:\t 59\n",
      "file b405025h_mode2.Filip.xml:\t 107\n",
      "file b405115g_mode2.Denis.xml:\t 35\n",
      "file b405149a_mode2.Denis.xml:\t 68\n",
      "file b405211k_mode2.Paola.xml:\t 29\n",
      "file b405233a_mode2.Petr.xml:\t 88\n",
      "file b405531d_mode2.Loris.xml:\t 117\n",
      "file b405543h_mode2.Andrew.xml:\t 95\n",
      "file b406051b_mode2.Nick.xml:\t 123\n",
      "file b406455k_mode2.Paola.xml:\t 128\n",
      "file b406562j_mode2.Denis.xml:\t 30\n",
      "file b406569g_mode2.Loris.xml:\t 114\n",
      "file b406695b_mode2.Nick.xml:\t 100\n",
      "file b406795a_mode2.Petr.xml:\t 64\n",
      "file b406899h_mode2.Donna.xml:\t 77\n",
      "file b406989g_mode2.Paola.xml:\t 71\n",
      "file b407082h_mode2.Petr.xml:\t 160\n",
      "file b407174c_mode2.Petr.xml:\t 124\n",
      "file b407279k_mode2.Andrew.xml:\t 144\n",
      "file b407283a_mode2.Nick.xml:\t 70\n",
      "file b407297a_mode2.Kamran.xml:\t 82\n",
      "file b407316a_mode2.Loris.xml:\t 73\n",
      "file b407358d_mode2.Denis.xml:\t 108\n",
      "file b407371c_mode2.Petr.xml:\t 52\n",
      "file b407500e_mode2.Denis.xml:\t 245\n",
      "file b407512a_mode2.Nick.xml:\t 104\n",
      "file b407728h_mode2.Denis.xml:\t 85\n",
      "file b408415b_mode2.Petr.xml:\t 91\n",
      "file b408506j_mode2.Denis.xml:\t 110\n",
      "file b408777a_mode2.Donna.xml:\t 58\n",
      "file b408850f_mode2.Donna.xml:\t 207\n",
      "file b409258a_mode2.Kamran.xml:\t 211\n",
      "file b409366f_mode2.Filip.xml:\t 81\n",
      "file b409443c_mode2.Loris.xml:\t 126\n",
      "file b409589h_mode2.Filip.xml:\t 148\n",
      "file b409641j_mode2.Filip.xml:\t 37\n",
      "file b409686j_mode2.Petr.xml:\t 34\n",
      "file b409840d_mode2.Petr.xml:\t 52\n",
      "file b410053k_mode2.Petr.xml:\t 106\n",
      "file b410112j_mode2.Denis.xml:\t 113\n",
      "file b410120k_mode2.Nick.xml:\t 35\n",
      "file b410283e_mode2.Denis.xml:\t 27\n",
      "file b410439k_mode2.Petr.xml:\t 30\n",
      "file b410905h_mode2.Filip.xml:\t 72\n",
      "file b410922h_mode2.Nick.xml:\t 107\n",
      "file b411001c_mode2.Petr.xml:\t 74\n",
      "file b411005f_mode2.Paola.xml:\t 104\n",
      "file b411044g_mode2.Kamran.xml:\t 68\n",
      "file b411047a_mode2.Donna.xml:\t 95\n",
      "file b411082j_mode2.Donna.xml:\t 35\n",
      "file b411335g_mode2.Loris.xml:\t 35\n",
      "file b411356j_mode2.Loris.xml:\t 69\n",
      "file b411500g_mode2.Paola.xml:\t 54\n",
      "file b411689e_mode2.Denis.xml:\t 54\n",
      "file b411757c_mode2.Paola.xml:\t 75\n",
      "file b411798k_mode2.Nick.xml:\t 87\n",
      "file b411977k_mode2.Donna.xml:\t 182\n",
      "file b411989d_mode2.Denis.xml:\t 72\n",
      "file b412414f_mode2.Loris.xml:\t 130\n",
      "file b412680g_mode2.Paola.xml:\t 128\n",
      "file b412883d_mode2.Kamran.xml:\t 172\n",
      "file b413535k_mode2.Denis.xml:\t 87\n",
      "file b413539c_mode2.Donna.xml:\t 45\n",
      "file b413603a_mode2.Andrew.xml:\t 105\n",
      "file b413672a_mode2.Nick.xml:\t 59\n",
      "file b413703e_mode2.Kamran.xml:\t 29\n",
      "file b414296a_mode2.Denis.xml:\t 158\n",
      "file b414459g_mode2.Filip.xml:\t 37\n",
      "file b414639e_mode2.Andrew.xml:\t 114\n",
      "file b414641g_mode2.Andrew.xml:\t 97\n",
      "file b414986f_mode2.Kamran.xml:\t 76\n",
      "file b415247f_mode2.Andrew.xml:\t 95\n",
      "file b415321a_mode2.Donna.xml:\t 77\n",
      "file b415563f_mode2.Andrew.xml:\t 240\n",
      "file b416924g_mode2.Andrew.xml:\t 125\n",
      "file b418708c_mode2.Kamran.xml:\t 42\n",
      "file b419201j_mode2.Kamran.xml:\t 61\n",
      "file b419490j_mode2.Kamran.xml:\t 93\n",
      "file b501029b_mode2.Andrew.xml:\t 53\n",
      "file b502789f_mode2.Andrew.xml:\t 69\n",
      "file b504178c_mode2.Andrew.xml:\t 105\n",
      "file b504407c_mode2.Andrew.xml:\t 73\n",
      "file b506075c_mode2.Kamran.xml:\t 67\n",
      "file b506280b_mode2.Andrew.xml:\t 101\n",
      "file b506644a_mode2.Andrew.xml:\t 46\n",
      "file b506790a_mode2.Andrew.xml:\t 188\n",
      "file b508092d_mode2.Andrew.xml:\t 123\n",
      "file b508229c_mode2.Andrew.xml:\t 157\n",
      "file b508281a_mode2.Andrew.xml:\t 95\n",
      "file b509164k_mode2.Kamran.xml:\t 63\n",
      "file b509263a_mode2.Kamran.xml:\t 41\n",
      "file b510884e_mode2.Andrew.xml:\t 52\n",
      "file b511334b_mode2.Andrew.xml:\t 98\n",
      "file b511710k_mode2.Kamran.xml:\t 44\n",
      "file b702933k_mode2.Andrew.xml:\t 96\n",
      "file b704599a_mode2.Loris.xml:\t 51\n",
      "file b707797a_mode2.Denis.xml:\t 170\n",
      "file b709062e_mode2.Filip.xml:\t 119\n",
      "\n",
      "Max. number of formulae is in file b403970j_mode2.Filip.xml, count:  271\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "results = {} # store the results in a dictionary\n",
    "pattern = \".*[a-zA-Z\\(\\)+\\[\\]-]+[0-9\\(\\)+\\[\\]-][a-zA-Z0-9\\(\\)+\\[\\]-]*.*\"\n",
    "\n",
    "# After inspecting the ART corpus, we see that we can expect chemical formulae to appear only between\n",
    "# <text></text> tags\n",
    "\n",
    "# We iterate over files in the ART_corpus directory\n",
    "for filename in os.listdir(\"ART_corpus/ART_Corpus/\"):\n",
    "    results[filename] = 0\n",
    "    tree = etree.parse('ART_corpus/ART_Corpus/' + filename)\n",
    "    for element in tree.xpath('//text'):\n",
    "        xml = etree.tostring(element,pretty_print=True).decode()       \n",
    "        temp = re.match(pattern, xml)\n",
    "        if temp:\n",
    "            results[filename] = results[filename] + 1\n",
    "    print(\"file \" + filename + \":\\t\", results[filename])\n",
    "    \n",
    "# work out which file contains the most formulae\n",
    "max_formulae = 0;\n",
    "max_file = \"\"\n",
    "for file in results:\n",
    "    if results[file] > max_formulae:\n",
    "        max_formulae = results[file]\n",
    "        max_file = file\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Max. number of formulae is in file \" + max_file + \", count: \", max_formulae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code prints the number of chemical formulae identified in each file. It is very likely that we got some false positives, so probably anything with a count below 30 should be discarded. For example, file b305670h_mode2.Paola.xml had only 12 cases where the regex gave a positive results. After manual inspection, we see that it doesn't contain chemical formulae as such, but some abbreviations such as '(CNTs)' could have triggered the regex.\n",
    "\n",
    "Max. number of formulae is in file b403970j_mode2.Filip.xml, count:  271"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to verify the pen and paper exercise:\n",
      "'refa' to 'fear':     4\n",
      "'drive' to 'brief':   4\n",
      "'drive' to 'divers':  3\n"
     ]
    }
   ],
   "source": [
    "# Minimum edit distance algorithm (Wagner-Fischer). Note that this isn't a very efficient algorithm (it runs in\n",
    "# exponential time, and n^2 space).\n",
    "\n",
    "def minEditDistance(string1, string2):\n",
    "    # this will be a 2D array to store the distance. Think of it as n*m matrix, which we initially fill with 0s\n",
    "    array = [[0 for i in range(len(string1)+1)] for j in range(len(string2)+1)]; \n",
    "    # fill the top row of the matrix\n",
    "    for j in range(len(string1) + 1):\n",
    "        array[0][j] = j\n",
    "        \n",
    "    # fill the first column\n",
    "    for i in range(len(string2) + 1):\n",
    "        array[i][0] = i\n",
    "        \n",
    "    for i in range(1, len(string2) + 1):\n",
    "        for j in range(1, len(string1) + 1):\n",
    "            if string1[j-1] == string2[i-1]:\n",
    "                array[i][j] = array[i-1][j-1]\n",
    "            else:\n",
    "                array[i][j] = min (min ((array[i-1][j-1] + 2), (array[i][j-1] + 1)), (array[i-1][j] + 1))\n",
    "        \n",
    "    return array[len(string2)][len(string1)]\n",
    "    \n",
    "print(\"to verify the pen and paper exercise:\")\n",
    "print(\"'refa' to 'fear':    \", minEditDistance(\"refa\", \"fear\"))\n",
    "print(\"'drive' to 'brief':  \", minEditDistance(\"drive\", \"brief\"))\n",
    "print(\"'drive' to 'divers': \", minEditDistance(\"drive\", \"divers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "r*efa*\n",
      "||||||\n",
      "*fe*ar\n",
      "'refa' to 'fear':  4\n",
      "\n",
      "d*rive*\n",
      "|||||||\n",
      "*bri*ef\n",
      "'drive' to 'brief':  4\n",
      "\n",
      "drive**\n",
      "|||||||\n",
      "d*ivers\n",
      "'drive' to 'divers':  3\n"
     ]
    }
   ],
   "source": [
    "# We ammend the above code to output the alignment, following the Needleman-Wunsch algorithm\n",
    "# https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm\n",
    "\n",
    "def editAlignment(string1, string2):\n",
    "    # this will be a 2D array to store the distance. Think of it as n*m matrix, which we initially fill with 0s\n",
    "    array = [[0 for i in range(len(string1)+1)] for j in range(len(string2)+1)]; \n",
    "    # this array records how we got to each stage\n",
    "    # i.e. each cell contains the coordinates of the cell from which we moved to this one in the array\n",
    "    alignment = [[[0, 0] for i in range(len(string1)+1)] for j in range(len(string2)+1)];\n",
    "    \n",
    "    # fill the top row of the matrix\n",
    "    for j in range(len(string1) + 1):\n",
    "        array[0][j] = j\n",
    "        \n",
    "    # fill the first column\n",
    "    for i in range(len(string2) + 1):\n",
    "        array[i][0] = i\n",
    "        \n",
    "    for i in range(1, len(string2) + 1):\n",
    "        for j in range(1, len(string1) + 1):\n",
    "            # dealing with the same letter in both strings\n",
    "            if string1[j-1] == string2[i-1]:\n",
    "                array[i][j] = array[i-1][j-1]\n",
    "                alignment[i][j] = [i-1, j-1]\n",
    "            else:\n",
    "                step = min (min ((array[i-1][j-1] + 2), (array[i][j-1] + 1)), (array[i-1][j] + 1))\n",
    "                array[i][j] = step\n",
    "                # recording which was the previous cell\n",
    "                if step == array[i-1][j-1] + 2:\n",
    "                    alignment[i][j] = [i-1, j-1]\n",
    "                elif step == array[i][j-1] + 1:\n",
    "                    alignment[i][j] = [i, j-1]\n",
    "                else:\n",
    "                    alignment[i][j] = [i-1,j]\n",
    "        \n",
    "    # Now we read off the path from the alignment array\n",
    "    backtrace = [[len(string2),len(string1)]]\n",
    "    while backtrace[0] != [0,0]:\n",
    "        backtrace.insert(0, alignment[backtrace[0][0]][backtrace[0][1]])\n",
    "        \n",
    "    # We convert it to three strings that we can then print to demonstrate alignment\n",
    "    top = \"\"\n",
    "    bot = \"\"\n",
    "    mid = \"\"\n",
    "    for i in range(1,len(backtrace)):\n",
    "        # if one of the indices is the same as in the previous step, we have only an insertion or deletion\n",
    "        if backtrace[i][0] == backtrace[i-1][0]:\n",
    "            top += string1[backtrace[i][1]-1]\n",
    "            mid += \"|\"\n",
    "            bot += \"*\"\n",
    "        elif backtrace[i][1] == backtrace[i-1][1]:\n",
    "            top += \"*\"\n",
    "            mid += \"|\"\n",
    "            bot += string2[backtrace[i][0]-1]\n",
    "        else:\n",
    "            # no operation, same letter in both strings\n",
    "            if string1[backtrace[i][1]-1] == string2[backtrace[i][0]-1]:\n",
    "                top += string1[backtrace[i][1]-1]\n",
    "                mid += \"|\"\n",
    "                bot += string2[backtrace[i][0]-1]\n",
    "            else:\n",
    "            # substitution, by personal preference displayed as insertion and deletion\n",
    "                top += string1[backtrace[i][1]-1]\n",
    "                mid += \"|\"\n",
    "                bot += \"*\"\n",
    "                top += \"*\"\n",
    "                mid += \"|\"\n",
    "                bot += string2[backtrace[i][0]-1]\n",
    "\n",
    "    print(\"\") # empty line\n",
    "    print(top)\n",
    "    print(mid)\n",
    "    print(bot)\n",
    "    return array[len(string2)][len(string1)]\n",
    "    \n",
    "print(\"'refa' to 'fear': \", editAlignment(\"refa\", \"fear\"))\n",
    "print(\"'drive' to 'brief': \", editAlignment(\"drive\", \"brief\"))\n",
    "print(\"'drive' to 'divers': \", editAlignment(\"drive\", \"divers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for counting the words (adding a new key if necessary)\n",
    "def addToCount(key, dictionary):\n",
    "    if key in dictionary:\n",
    "        dictionary[key] = dictionary[key] + 1\n",
    "    else:\n",
    "        dictionary[key] = 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing test_corpus...\n",
      "Number of tokens:  85\n",
      "3 most common unigrams: \n",
      "[('</s>', 15), ('<s>', 15), ('fus', 8)]\n",
      "5 most common bigrams: \n",
      "[('fus ro', 8), ('dah </s>', 8), ('ro dah', 8), ('<s> fus', 8), ('Sam </s>', 3)]\n",
      "3 randomly generated sentences of length <5:\n",
      "<s> fus ro dah </s>\n",
      "<s> This is the first test\n",
      "<s> fus ro dah </s>\n",
      "\n",
      "Analysing Sam_corpus...\n",
      "Number of tokens:  25\n",
      "3 most common unigrams: \n",
      "[('I', 4), ('</s>', 4), ('Sam', 4)]\n",
      "5 most common bigrams: \n",
      "[('I am', 3), ('Sam </s>', 3), ('<s> I', 3), ('am Sam', 2), ('Sam I', 1)]\n",
      "3 randomly generated sentences of length <5:\n",
      "<s> I am </s>\n",
      "<s> Sam </s>\n",
      "<s> I am Sam </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Program that computes unsmoothed bigrams and unigrams\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import operator\n",
    "from lxml import etree\n",
    "\n",
    "def ngrams(directory):\n",
    "    # Store ngrams in dictionaries\n",
    "    unigrams = {}\n",
    "    # bigrams are stored in the format: \"word next_word\" : count\n",
    "    bigrams = {} \n",
    "    \n",
    "    # Parse the corpus\n",
    "    for filename in os.listdir(directory):\n",
    "        tree = etree.parse(directory + filename)\n",
    "        for element in tree.xpath('//s'):\n",
    "            xml = etree.tostring(element,pretty_print=True).decode() # pretty_print strips the b' flag at the start\n",
    "            temp = re.split('; |,|, |\\.|\\. |!|\\(|\\)|\\s+|\\n', xml)\n",
    "            for word in temp:\n",
    "                if word != \"\":\n",
    "                    # first deal with <s> and </s>, before we ignore all other tags\n",
    "                    if word == \"<s>\":\n",
    "                        unigrams = addToCount(\"<s>\", unigrams)\n",
    "                    elif word == \"</s>\":\n",
    "                        unigrams = addToCount(\"</s>\", unigrams)\n",
    "                    elif re.match('.*[<>#=].*', word): # ignore tags\n",
    "                        continue\n",
    "                    else:\n",
    "                        unigrams = addToCount(word, unigrams)\n",
    "                        \n",
    "            # Now to bigrams\n",
    "            for i in range(len(temp)-2):\n",
    "                word = temp[i]\n",
    "                if word != \"\":\n",
    "                    # first deal with <s> and </s>, before we ignore all other tags\n",
    "                    if word == \"<s>\":\n",
    "                        bigrams = addToCount(\"<s> \" + temp[i+1], bigrams)\n",
    "                    elif re.match('.*[<>#=].*', word): # ignore tags\n",
    "                        continue\n",
    "                    else:\n",
    "                        bigrams = addToCount(word + \" \" + temp[i+1], bigrams)\n",
    "                        \n",
    "    tokens = 0 # count how many tokens we see\n",
    "    for word in unigrams:\n",
    "        tokens = tokens + unigrams[word]\n",
    "    \n",
    "    top_unigrams = dict(sorted(unigrams.items(), key=operator.itemgetter(1), reverse=True)[:3])\n",
    "    top_unigrams = sorted(top_unigrams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    top_bigrams = dict(sorted(bigrams.items(), key=operator.itemgetter(1), reverse=True)[:5])\n",
    "    top_bigrams = sorted(top_bigrams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"Analysing \" + directory[:-1] + \"...\")\n",
    "    print(\"Number of tokens: \", tokens)\n",
    "    print(\"3 most common unigrams: \")\n",
    "    print(top_unigrams)\n",
    "    print(\"5 most common bigrams: \")\n",
    "    print(top_bigrams)\n",
    "    print(\"3 randomly generated sentences of length <5:\")\n",
    "    print(generateSentence(5, bigrams, unigrams))\n",
    "    print(generateSentence(5, bigrams, unigrams))\n",
    "    print(generateSentence(5, bigrams, unigrams))\n",
    "    print(\"\")\n",
    "   \n",
    "# Note: we are passing bigrams and unigrams dictionaries as parameters to all of these functions, to make sure\n",
    "# that they take the ones we want, and not the ones from the next code snippet (this sometimes happens for \n",
    "# unknown reasons)\n",
    "\n",
    "# function to generate random sentences\n",
    "def generateSentence(length, bigrams, unigrams):\n",
    "    sentence = \"<s>\"\n",
    "    word = \"<s>\"\n",
    "    for i in range(length):\n",
    "        word = chooseNextWord(word, bigrams, unigrams)\n",
    "        if word: # we need to make sure that we're not in a dead end (i.e. trying to find a word to follow </s>)\n",
    "            sentence = sentence + \" \" + word\n",
    "            i = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "# function to trim the bigrams dictionary only to entries starting with the given word\n",
    "def trimDict(word, bigrams, unigrams):\n",
    "    trimmed_bigrams = {}\n",
    "    for bigram in bigrams:\n",
    "        if bigram.split()[0] == word:\n",
    "            trimmed_bigrams[bigram] = bigrams[bigram]/unigrams[word]\n",
    "    return trimmed_bigrams\n",
    "\n",
    "# function to pick the next word based on probabilities of bigrams\n",
    "def chooseNextWord(previous_word, bigrams, unigrams):\n",
    "    trimmed_bigrams = trimDict(previous_word, bigrams, unigrams)\n",
    "    total = 0\n",
    "    for word in trimmed_bigrams:\n",
    "        total = total + trimmed_bigrams[word]\n",
    "    rand = random.uniform(0, total)\n",
    "    temp = 0\n",
    "    for word in trimmed_bigrams:\n",
    "        if temp + trimmed_bigrams[word] >= rand:\n",
    "            return word.split()[1]\n",
    "        temp = temp + trimmed_bigrams[word]\n",
    "        \n",
    "# test_corpus is a collection of 3 very simple xml files\n",
    "ngrams(\"test_corpus/\")\n",
    "# Sam_corpus contains a single file with the corpus from question 1\n",
    "ngrams(\"Sam_corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. & 4.\n",
    " After inspecting the ART corpus, we see that the actual text appears between `<text></text>` tags, while the \n",
    " sentence tags are its parents and contain other elements. Moreover, the sentence tags contain unique ids. Thus,\n",
    " counting them would not give a meaningful result.\n",
    " Therefore we have chosen to use `<text></text>` tags instead of `<s></s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ART_corpus contains 1062151 words, 31862 tokens, and 264014 bigrams.\n",
      "50 most common unigrams: \n",
      "[('the', 77762), ('of', 43300), ('</text>', 40180), ('<text>', 40180), ('and', 25836), ('in', 21634), ('to', 20156), ('a', 16658), ('is', 14340), ('for', 11218), ('The', 10813), ('with', 9592), ('that', 7894), ('by', 7528), ('are', 7130), ('at', 6193), ('as', 5995), ('be', 5916), ('was', 5906), ('from', 5182), ('on', 4658), ('1', 4626), ('were', 4171), ('this', 3856), ('which', 3581), ('0', 3537), ('2', 3528), ('In', 3317), ('an', 3315), ('Fig', 3160), ('can', 2727), ('have', 2633), ('energy', 2535), ('not', 2417), ('3', 2402), ('we', 2380), ('or', 2368), ('5', 2270), ('This', 2254), ('two', 2248), ('between', 2242), ('been', 2092), ('than', 2072), ('has', 2010), ('4', 1895), ('also', 1865), ('it', 1847), ('using', 1813), ('these', 1798), ('state', 1786)]\n",
      "50 most common bigrams: \n",
      "[('of the', 15683), ('<text> The', 10526), ('in the', 7428), ('to the', 4845), ('for the', 3540), ('<text> In', 3232), ('Fig ', 3147), ('and the', 2957), ('that the', 2554), ('with the', 2511), ('on the', 2460), ('<text> This', 2209), ('from the', 2045), ('can be', 1766), ('to be', 1659), ('by the', 1542), ('of a', 1455), ('at the', 1235), ('in Fig', 1232), ('with a', 1189), ('1 ', 1182), ('<text> For', 1168), ('in a', 1156), ('<text> It', 1105), ('is the', 1098), ('<text> A', 1049), ('due to', 999), ('<text> We', 988), ('2 ', 983), ('between the', 968), ('as a', 957), ('to a', 936), ('the same', 884), ('has been', 879), ('In the', 873), ('have been', 826), ('<text> However', 804), ('<text> As', 761), ('as the', 758), ('However ', 753), ('shown in', 731), ('it is', 724), ('eqn ', 691), ('is a', 681), ('the two', 680), ('which is', 677), ('<text> These', 675), ('3 ', 671), ('number of', 632), ('by a', 631)]\n",
      "\n",
      "Here are 4 randomly generated sentences of length 10:\n",
      "<text> There are easy optical spectroscopy by Falk\n",
      "<text> The calculated in the same purpose to 1273 K where\n",
      "<text> This point tested against the chopper configuration space being that\n",
      "<text> A suitable not unambiguously assigned to examine three days depending\n"
     ]
    }
   ],
   "source": [
    "# We're asked for a program that computes unsmoothed unigrams and bigrams\n",
    "# This is for the most part a copy-paste of the above code. The key difference is in how we deal with start and end \n",
    "# of sentence tags, as they are not as nicely formatted as we'd like\n",
    "\n",
    "import re\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "unigrams = {\"<text>\":0, \"</text>\":0} # store the results in a dictionary\n",
    "bigrams = {}\n",
    "# note: we keep unigrams and bigrams case-sensitive, which will hopefully help with generating sentences later \n",
    "\n",
    "# We iterate over files in the ART_corpus directory\n",
    "for filename in os.listdir(\"ART_corpus/ART_Corpus/\"):\n",
    "    tree = etree.parse('ART_corpus/ART_Corpus/' + filename)\n",
    "    for element in tree.xpath('//text'):\n",
    "        xml = etree.tostring(element,pretty_print=True).decode()       \n",
    "        temp = re.split('; |,|, |\\.|/|\\. |!|\\(|\\)|\\s+|\\n', xml)\n",
    "        # the first word listed will be \"<text>word\" and last will be \"...word.</text>\", so we deal with those\n",
    "        unigrams[\"<text>\"] = unigrams[\"<text>\"] + 1\n",
    "        unigrams[\"</text>\"] = unigrams[\"</text>\"] + 1\n",
    "        temp[0] = temp[0][6:] # strip \"<text>\" off the first word\n",
    "        # Unigrams\n",
    "        for word in temp:\n",
    "            if word != \"\":\n",
    "                if re.match('.*[<>#=].*', word): # ignore tags\n",
    "                    continue\n",
    "                else:\n",
    "                    unigrams = addToCount(word, unigrams)\n",
    "                        \n",
    "        # Now onto bigrams\n",
    "        # since we stripped <text> tag, we need to re-add it manually\n",
    "        bigrams = addToCount(\"<text> \" + temp[0], bigrams)\n",
    "        for i in range(len(temp)-2):\n",
    "            word = temp[i]\n",
    "            if word != \"\":\n",
    "                if re.match('.*[<>#=].*', word) or re.match('.*[<>#=].*', temp[i+1]): # ignore tags\n",
    "                    continue\n",
    "                else:\n",
    "                    bigrams = addToCount(word + \" \" + temp[i+1], bigrams)\n",
    "\n",
    "tokens = 0 # count how many tokens we see\n",
    "for word in unigrams:\n",
    "    tokens = tokens + unigrams[word]   \n",
    "\n",
    "unigrams_prob = {}\n",
    "for word in unigrams:\n",
    "    unigrams_prob[word] = unigrams[word]/tokens\n",
    "    \n",
    "bigrams_prob = {}\n",
    "for word in bigrams:\n",
    "    words = word.split()\n",
    "    bigrams_prob[word] = bigrams[word]/unigrams[words[0]] \n",
    "\n",
    "# ****************** #\n",
    "# **    Part 4    ** #\n",
    "# ****************** #\n",
    "\n",
    "# function to generate random sentences\n",
    "def generateSentence(length, bigrams, unigrams):\n",
    "    # I assumed that sentences should start with <text> tag.\n",
    "    sentence = \"<text>\"\n",
    "    word = \"<text>\"\n",
    "    for i in range(length):\n",
    "        word = chooseNextWord(word, bigrams, unigrams)\n",
    "        if word: # we need to make sure that we're not in a dead end (i.e. trying to find a word to follow </s>)\n",
    "            sentence = sentence + \" \" + word\n",
    "            i = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return sentence\n",
    "\n",
    "# function to trim the bigrams dictionary only to entries starting with the given word\n",
    "def trimDict(word, bigrams, unigrams):\n",
    "    trimmed_bigrams = {}\n",
    "    for bigram in bigrams:\n",
    "        if bigram.split()[0] == word:\n",
    "            trimmed_bigrams[bigram] = bigrams[bigram]/unigrams[word]\n",
    "    return trimmed_bigrams\n",
    "\n",
    "# function to pick the next word based on probabilities of bigrams\n",
    "def chooseNextWord(previous_word, bigrams, unigrams):\n",
    "    trimmed_bigrams = trimDict(previous_word, bigrams, unigrams)\n",
    "    total = 0\n",
    "    for word in trimmed_bigrams:\n",
    "        total = total + trimmed_bigrams[word]\n",
    "    rand = random.uniform(0, total)\n",
    "    temp = 0\n",
    "    for word in trimmed_bigrams:\n",
    "        if (temp + trimmed_bigrams[word] >= rand) & (len(word.split()) == 2):\n",
    "            return word.split()[1]\n",
    "        temp = temp + trimmed_bigrams[word]\n",
    "\n",
    "top_unigrams = dict(sorted(unigrams.items(), key=operator.itemgetter(1), reverse=True)[:50])\n",
    "top_unigrams = sorted(top_unigrams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "top_bigrams = dict(sorted(bigrams.items(), key=operator.itemgetter(1), reverse=True)[:50])\n",
    "top_bigrams = sorted(top_bigrams.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "print(\"The ART_corpus contains \" + str(tokens) + \" words, \" + str(len(unigrams)) + \" tokens, and \" + str(len(bigrams)) + \" bigrams.\")\n",
    "print(\"50 most common unigrams: \")\n",
    "print(top_unigrams)\n",
    "print(\"50 most common bigrams: \")\n",
    "print(top_bigrams)\n",
    "print(\"\")        \n",
    "print(\"Here are 4 randomly generated sentences of length 10:\")\n",
    "print(generateSentence(10, bigrams, unigrams))\n",
    "print(generateSentence(10, bigrams, unigrams))\n",
    "print(generateSentence(10, bigrams, unigrams))\n",
    "print(generateSentence(10, bigrams, unigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
